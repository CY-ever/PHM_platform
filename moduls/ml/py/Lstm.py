# -*- coding: utf-8 -*-
"""Lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yl9uTX4lMUMlCZkKR43zkDmK8SWZsYDH
"""

# from google.colab import drive
# drive.mount('/content/drive')

import tensorflow as tf
import pandas as pd
from sklearn.metrics import r2_score
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np
from random import choice
# from keras.utils.np_utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dropout, Dense, LSTM, Input
from sklearn.model_selection import train_test_split


class paramLSTM:
    units = 100
    activation = 'tanh'
    recurrent_activation = 'sigmoid'
    use_bias = True
    kernel_initializer = 'glorot_uniform'
    recurrent_initializer = 'orthogonal'
    bias_initializer = 'zeros'
    unit_forget_bias = True
    kernel_regularizer = None
    recurrent_regularizer = None
    bias_regularizer = None
    activity_regularizer = None
    kernel_constraint = None
    recurrent_constraint = None
    bias_constraint = None
    dropout = 0
    recurrent_dropout = 0.0
    implementation = 2  #
    return_sequences = True  # return_sequences=False,
    return_state = False
    go_backwards = False
    stateful = False
    time_major = False
    unroll = False


class paramDropout:
    rate = 0.2
    noise_shape = None
    seed = None


class paramDense:
    units = 1
    activation = 'sigmoid'
    use_bias = True
    kernel_initializer = 'glorot_uniform'
    bias_initializer = 'zeros'
    kernel_regularizer = None
    bias_regularizer = None
    activity_regularizer = None
    kernel_constraint = None
    bias_constraint = None


class compileOption:
    optimizer = 'rmsprop'
    loss = None
    metrics = None
    loss_weights = None,
    weighted_metrics = None
    run_eagerly = None


class fitOption:
    x = None
    y = None
    batch_size = None
    epochs = 1
    verbose = 1
    callbacks = None
    validation_split = 0.0
    validation_data = None
    shuffle = True
    class_weight = None
    sample_weight = None
    initial_epoch = 0
    steps_per_epoch = None
    validation_steps = None
    validation_batch_size = None
    validation_freq = 1
    max_queue_size = 10
    workers = 1
    use_multiprocessing = False


# add a LSTM layer in the model
def L(typ, Param, model):
    if 'LSTM' == typ:
        model.add(tf.keras.layers.LSTM(Param.units,
                                       implementation=Param.implementation,
                                       return_sequences=Param.return_sequences,
                                       ))
        # model.add(tf.keras.layers.LSTM(Param.units,
        #                                activation=Param.activation,
        #                                recurrent_activation=Param.recurrent_activation,
        #                                use_bias=Param.use_bias,
        #                                kernel_initializer=Param.kernel_initializer,
        #                                recurrent_initializer=Param.recurrent_initializer,
        #                                bias_initializer=Param.bias_initializer,
        #                                unit_forget_bias=Param.unit_forget_bias,
        #                                kernel_regularizer=Param.kernel_regularizer,
        #                                recurrent_regularizer=Param.recurrent_regularizer,
        #                                bias_regularizer=Param.bias_regularizer,
        #                                activity_regularizer=Param.activity_regularizer,
        #                                kernel_constraint=Param.kernel_constraint,
        #                                recurrent_constraint=Param.recurrent_constraint,
        #                                bias_constraint=Param.bias_constraint,
        #                                dropout=Param.dropout,
        #                                recurrent_dropout=Param.recurrent_dropout,
        #                                implementation=Param.implementation,
        #                                return_sequences=Param.return_sequences,
        #                                return_state=Param.return_state,
        #                                go_backwards=Param.go_backwards,
        #                                stateful=Param.stateful,
        #                                time_major=Param.time_major,
        #                                unroll=Param.unroll
        #                                ))


# add a Dense or Dropout layer in the model
def D(typ, Param, model):
    if 'Dense' == typ:
        # """Param.units/4"""
        # model.add(tf.keras.layers.Dense(Param.units,
        #                                 ))
        model.add(tf.keras.layers.Dense(1,
                                        ))
        # model.add(tf.keras.layers.Dense(Param.units
        #                                 , activation=Param.activation
        #                                 , use_bias=Param.use_bias
        #                                 , kernel_initializer=Param.kernel_initializer
        #                                 , bias_initializer=Param.bias_initializer
        #                                 , kernel_regularizer=Param.kernel_regularizer
        #                                 , bias_regularizer=Param.bias_regularizer
        #                                 , activity_regularizer=Param.activity_regularizer
        #                                 , kernel_constraint=Param.kernel_constraint
        #                                 , bias_constraint=Param.bias_constraint
        #                                 ))
    # if 'Dropout' == typ:
    #     model.add(tf.keras.layers.Dropout(Param.rate
    #                                       , noise_shape=Param.noise_shape
    #                                       , seed=Param.seed))


def initialParam(results):  # results=[2, 50, 100, 0, 0.2, 10, 128, 'softmax', 'adam', 'categorical_crossentropy']
    LSTM_num = results[0]
    Dense_num = 1
    Dropout_num = 1
    typList = []
    ParamList = []
    for i in range(0, LSTM_num):
        # Add a layer of LSTM
        typList.append('LSTM')
        # Initialize the parameters of the LSTM layer
        param = paramLSTM()
        num = results[i + 1]
        param.units = num
        # The last layer of return_sequences is changed to False, otherwise an error will be reported
        if i == LSTM_num - 1:
            param.return_sequences = False
        # Add the parameters of the modified LSTM layer
        ParamList.append(param)

    # Add a dropout layer
    typList.append('Dropout')
    # Add a Dense layer
    typList.append('Dense')

    # Initialize the parameters of the Dropout layer
    param_Dropout = paramDropout()
    rate = results[4]
    # Modify the rate of the dropout layer
    param_Dropout.rate = rate
    # Add the parameters of the modified Dropout layer
    ParamList.append(param_Dropout)

    # Initialize the parameters of the Dense layer
    param_Dense = paramDense()
    param_Dense.units = 4
    activation = results[7]
    param_Dense.activation = activation

    # Add Dense parameters
    ParamList.append(param_Dense)
    # Finally we get typList and ParamList

    # Initialize the parameters of complie()

    compileParam = compileOption()
    # Random optimizers
    optimizer = results[8]
    compileParam.optimizer = optimizer
    loss = results[9]
    compileParam.loss = loss
    compileParam.metrics = 'accuracy'

    # Initialize the parameters of fit()

    fitParam = fitOption()
    # fitParam.x=x_train
    # fitParam.y=y_train
    epochs = results[5]
    fitParam.epochs = epochs
    batch_size = results[6]
    fitParam.batch_size = batch_size
    print(typList)
    return typList, ParamList, compileParam, fitParam


def fun_add(typ, Param, model):
    if 'L' == typ[0]:  # Determine which type of layer is according to the first letter of typ.
        L(typ, Param, model)  # If typ=LSTM, typ[0]=L, then go directly to the function starting with L to find
    if 'D' == typ[0]:
        D(typ, Param, model)


def fun_compile(model, compileOption):  # compileOption is the object that receives the front-end user parameters
    model.compile(optimizer=compileOption.optimizer
                  , loss=compileOption.loss
                  , metrics=compileOption.metrics
                  , loss_weights=compileOption.loss_weights
                  , weighted_metrics=compileOption.weighted_metrics
                  , run_eagerly=compileOption.run_eagerly)


def fun_fit(model, fitOption):  # training
    model.fit(fitOption.x,
              fitOption.y
              , batch_size=fitOption.batch_size
              , epochs=fitOption.epochs
              , verbose=fitOption.verbose
              , callbacks=fitOption.callbacks
              , validation_split=fitOption.validation_split
              , validation_data=fitOption.validation_data
              , shuffle=fitOption.shuffle
              , class_weight=fitOption.class_weight
              , sample_weight=fitOption.sample_weight
              , initial_epoch=fitOption.initial_epoch
              , steps_per_epoch=fitOption.steps_per_epoch
              , validation_steps=fitOption.validation_steps
              , validation_batch_size=fitOption.validation_batch_size
              , validation_freq=fitOption.validation_freq
              , max_queue_size=fitOption.max_queue_size
              , workers=fitOption.workers
              , use_multiprocessing=fitOption.use_multiprocessing
              )


def evalute_model(model, x_val, y_val, batch_size):  # Evaluation the accuracy of the model
    loss, accuracy = model.evaluate(x_val, y_val, batch_size, verbose=1)
    return loss, accuracy


def train_model(x_train, y_train, results, rul_pre=False):
    # results = [layer_count,
    #            units1,
    #            units2,
    #            units3,
    #            dropoutRate,
    #            epochs,
    #            batchSize,
    #            denseActivation,
    #            optimizer,
    #            loss]
    if rul_pre:
        model = Sequential()
        model.add(Input(shape=(x_train.shape[1], x_train.shape[2])))
        for i in range(results[0]):
            if i == 0 and results[0] == 1:
                return_sequences = False
            elif i != (results[0]-1):       #i == 0 or i:
                return_sequences = True
            else:
                return_sequences = False
            model.add(tf.keras.layers.LSTM(results[i + 1],
                                           activation=results[7],                      #'softmax',
                                           return_sequences=return_sequences,
                                           ))
        model.add(tf.keras.layers.Dropout(results[4],
                                          ))
        model.add(tf.keras.layers.Dense(1,
                                        activation='sigmoid',
                                        ))
        model.compile(optimizer=results[8], loss=results[9])
        model.summary()
        model.fit(x_train, y_train, epochs=results[5], batch_size=results[6], verbose=1)

    else:
        model = Sequential()
        print("x_train:", x_train.shape)
        model.add(Input(shape=(x_train.shape[1], x_train.shape[2])))

        for i in range(results[0]):
            if i == 0:
                return_sequences = True
            else:
                return_sequences = False
            model.add(tf.keras.layers.LSTM(results[i + 1],
                                           activation='relu',
                                           return_sequences=return_sequences,
                                           ))
        if results[0] == 1:
            model.add(keras.layers.GlobalAveragePooling1D())
        model.add(tf.keras.layers.Dropout(results[4],
                                          ))
        y_train_len = len(y_train[0])
        print("y_train_len:", y_train_len)
        model.add(tf.keras.layers.Dense(y_train_len,
                                        activation='softmax',
                                        ))
        model.compile(optimizer=results[8],
                      loss=results[9],
                      metrics=['accuracy']
                      )
        model.summary()
        model.fit(x_train, y_train, epochs=results[5], batch_size=results[6], verbose=1)

    return model

    # # Call all the above methods to complete the training and return the accuracy
    # tf.random.set_seed(42)
    # fitOption.x = x_train
    # fitOption.y = y_train
    # Shape = x_train.shape
    # model = Sequential()
    # model.add(Input(shape=(576, 1)))
    # # model.add(Input(shape=(1, 576)))
    # # model.add(Input(shape=(576)))
    # # model.add(Input(shape=Shape))
    # typList, ParamList, compileParam, fitParam = initialParam(results)
    # length = len(typList)
    # for i in range(length):
    #     fun_add(typList[i], ParamList[i], model)
    # fun_compile(model, compileOption)
    # model.summary()
    # fun_fit(model, fitParam)
    # # accuracy = evalute_model(model, x_val, y_val, 64)[1]
    # # return accuracy
    # return model


def get_accuracy(x_train, x_val, y_train, y_val, results,
                 rul_pre=False):  # The final function, execute this one to get the result

    # print("y_train", y_train)
    model = train_model(x_train, y_train, results, rul_pre=rul_pre)  # Call the above train_model to return a precision
    pre = model.predict(x_val)
    # print("pre", pre)
    # pre = pre / np.max(pre)


    # Shape, x_train, y_train, x_val, y_val = data  # Load data set
    # typList, ParamList, compileParam, fitParam = initialParam(results)  # initialization
    # fitParam.x = x_train
    # fitParam.y = y_train
    # final_loss = model.history['val_loss'][-1]
    # loss = (int(final_loss * 10000) / 10000)
    # print(loss)
    # accuracy = 1 - loss
    if rul_pre:
        pre[pre > 1] = 1
        score = r2_score(y_val, pre)
        print("成绩:", score)
        # print("y_val", y_val.shape)
        # print("pre", y_val.shape)
        y = (y_val, pre)
    else:
        pre = list(pre)
        pre_new = []
        for i in pre:
            i[np.where(i == np.max(i))] = 1
            i[np.where(i != np.max(i))] = 0
            # np.where(i==np.max(i), 1, 0)
            pre_new.append(i)
        pre = np.array(pre_new)

        from sklearn.metrics import accuracy_score
        score = accuracy_score(y_val, pre)
        print("成绩:", score)

        pre_use = np.argmax(pre, axis=1)
        y_test_use = np.argmax(y_val, axis=1)

        y = (y_test_use, pre_use)

    return y, score
